{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition Using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and submit your python codes in “Jupyter Notebook” to perform the following tasks. Make sure to provide proper descriptions as MarkDown for each section of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a.__\n",
    "\n",
    "Download the dataset “Face” from CSNS. Check out the dataset. Open some of the jpg images. This is the Oivetti database of face images from AT&T research lab. It includes 400 faces (64x64 pixels) from 40 people (10 images per person).\n",
    "You have to also download the csv file that includes the labels of the images (the label is person’s ID). The goal is to build a Face Recognition algorithm to recognize each person using PCA dimensionality reduction and a non-linear SVM.\n",
    "you can use:\n",
    "\n",
    "`mpimg.imread(file_name)` to load an image, and\n",
    "\n",
    "`plt.imshow(image_name, cmap=plt.cm.gray)` to show an image (This is a little different from what we had in HW2!).\n",
    "\n",
    "Add `%matplotlib inline` at top of your code to make sure that the images will be shown inside the Jupyter explorer page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b.__\n",
    "\n",
    "Build the feature matrix and label vector: Each image is considered as a data sample with pixels as features. Thus, to build the feature table you have to convert each 64x64 image into a row of the feature matrix with 4096 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c.__\n",
    "\n",
    "Normalize each column of your feature matrix (This is required!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d.__\n",
    "\n",
    "Use sklearn functions to split the Normalized dataset into testing and training sets with the\n",
    "following parameters: `test_size=0.25`, `random_state=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e.__\n",
    "\n",
    "The dimensionality of the data samples is 4096. Use PCA to reduce the dimensionality from 4096 to 50 (i.e. only 50 principal components!). You should “fit” your PCA on your training set only, and then use this fitted model to “transform” both training and testing sets (When you finish this step, the number of columns in your testing and training sets should be 50). We will cover the details of PCA in next session of class. For now, you can use this format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from sklearn.decomposition import PCA`\n",
    "\n",
    "`k = 50` # (k is the number of components (new features) after dimensionality reduction)\n",
    "\n",
    "`my_pca = PCA(n_components = k)`\n",
    "\n",
    "__#X_Train is feature matrix of training set before dimensionality reduction,__\n",
    "\n",
    "__#X_Train_New is feature matrix of training set after dimensionality reduction:__\n",
    "\n",
    "`X_Train_new = my_pca.fit_transform(X_Train)`\n",
    "\n",
    "`X_Test_new = my_pca.transform(X_Test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f.__\n",
    "\n",
    "Design and Train a non-linear SVM classifier to recognize the face based on the training dataset that you built in part _(d)_.\n",
    "\n",
    "Use:\n",
    "\n",
    "`SVC(C=1, kernel='rbf', gamma=0.0005, random_state=1)`\n",
    "\n",
    "Then, test your SVM on testing set (from part _(d)_ ), and calculate and report the accuracy.\n",
    "\n",
    "Also, calculate and report the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__g.__\n",
    "\n",
    "Now, use `GridSearchCV` to find the best value for parameter `C` in your SVM.\n",
    "\n",
    "Search in this list: `[0.1, 1, 10, 100, 1e3, 5e3, 1e4, 5e4, 1e5]`.\n",
    "\n",
    "Remember that we want to use cross-validation method (`GridSearchCV`) to find the best `C`.\n",
    "\n",
    "Thus, you can again merge `X_train_new` and `X_test_new` (after dimensionality reduction), and also merge `y_train` and `y_test`, and then use `GridSearchCV` with 10-fold cross validation to find `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
